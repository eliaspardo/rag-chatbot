# RAG Chatbot for Self-Study

A self-study AI-powered chatbot that uses Retrieval-Augmented Generation (RAG) and adapts to any domain or subject matter.

## Features

### üéì Domain Expert Mode

- Ask questions about the provided PDF documents
- Get contextual answers based on your imported PDF materials
- Conversational interface with chat history
- Configurable for any domain (medical, legal, technical, academic, etc.)

### üìù Exam Prep Mode

- Request quiz questions on specific topics or sections
- Receive immediate feedback on your answers
- Adaptive questioning to avoid repetition
- Customizable for any exam or certification

## Technology Stack

- **LLM**: Together AI
- **Vector Database**: FAISS
- **Embeddings**: HuggingFace Sentence Transformers
- **Framework**: LangChain
- **PDF Processing**: PyMuPDF (fitz)

## Installation

### Prerequisites

- Python 3.8 or higher
- Together AI API key

### Setup

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd rag-chatbot
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Create environment file**

   ```bash
   cp .env.example .env
   ```

4. **Configure environment variables**

   Edit the `.env` file and configure your chatbot:

   ```env
   # Required Configurations

   # Domain Configuration - Customize for your use case
   CHATBOT_ROLE=expert tutor
   USE_CASE=learn from the provided materials

   # Together AI API Key
   TOGETHER_API_KEY=your_together_ai_api_key_here

   # Path to PDF file
   PDF_PATH=data/your_document.pdf

   # Optional Configurations
   ...
   ```

## Usage

### Running the Application

```bash
cd src
python main.py
```

### First Run

On the first run, the application will:

1. Process your PDF document
2. Split text into chunks
3. Create embeddings using HuggingFace Sentence Transformers
4. Store vectors in FAISS database

Subsequent runs will load the existing vector database. Delete the database if you want to use a different document.

### Operational Modes

#### Domain Expert Mode (Option 1)

```
‚ùì Your question: What are the key principles of risk-based testing?
üí° Answer: Based on the context, risk-based testing involves...
```

#### Exam Prep Mode (Option 2)

```
üß† Section / topic: Risk Management
‚ùì Your question: What are the main categories of product risks in software testing?
‚ùì Your answer: Functional failures and quality issues
üí° Answer: Good start! The main categories include functional failures...
```

## Project Structure

```
rag-chatbot/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Main application entry point
‚îÇ   ‚îú‚îÄ‚îÄ domain_expert.py   # Domain expert logic
‚îÇ   ‚îú‚îÄ‚îÄ exam_prep.py       # Exam prep logic
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py         # System and sentence condense prompts
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ your_document.pdf  # Place any file you want to use for context here
‚îî‚îÄ‚îÄ faiss_db/              # Vector store - autogenerated upon first run.
```

## Configuration Options

| Variable           | Default                                         | Description                           |
| ------------------ | ----------------------------------------------- | ------------------------------------- |
| `CHATBOT_ROLE`     | expert tutor                                    | Chatbot's role                        |
| `USE_CASE`         | learn from the provided materials               | Learning goal                         |
| `TOGETHER_API_KEY` | -                                               | Your Together AI API key (required)   |
| `MODEL_NAME`       | `mistralai/Mistral-7B-Instruct-v0.1`            | LLM model to use                      |
| `PDF_PATH`         | -                                               | Path to your PDF document             |
| `EMBEDDING_MODEL`  | `sentence-transformers/paraphrase-MiniLM-L3-v2` | Embedding model                       |
| `DB_DIR`           | `faiss_db`                                      | Directory for vector database         |
| `CHUNK_SIZE`       | `500`                                           | Text chunk size for processing        |
| `CHUNK_OVERLAP`    | `50`                                            | Overlap between text chunks           |
| `RETRIEVAL_K`      | `4`                                             | Number of relevant chunks to retrieve |
| `TEMPERATURE`      | `0.3`                                           | LLM temperature (creativity)          |
| `MAX_TOKENS`       | `512`                                           | Maximum tokens in LLM response        |

## Dependencies

Listed in `requirements.txt` file.

## Commands

During chat sessions:

- Type your question or topic normally
- `mode` - Return to operational mode selection
- `quit`, `exit`, `no`, or `stop` - Exit the application
- `Ctrl+C` - Force quit

## Troubleshooting

### Common Issues

**PDF Processing Errors**

- Ensure your PDF path is correct in `.env`
- Check that the PDF is readable and not password-protected

**Together AI API Errors**

- Verify your API key is valid and has sufficient credits
- Check network connectivity

**Memory Issues**

- Reduce `CHUNK_SIZE` if processing large documents
- Consider using smaller embedding models

**Vector Database Issues**

- Delete the `faiss_db` directory to rebuild the database
- Ensure sufficient disk space

### Debugging

Enable verbose mode by uncommenting the `verbose=True` lines in the chain configurations:

```python
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True,  # Uncomment for debugging
)
```

## Support

For issues create an issue in the GitHub repository.
