# RAG Chatbot for Self-Study

A self-study AI-powered chatbot that uses Retrieval-Augmented Generation (RAG) and adapts to any domain or subject matter.

## Features

### üéì Domain Expert Mode

- Ask questions about the provided PDF documents
- Get contextual answers based on your imported PDF materials
- Conversational interface with chat history
- Configurable for any domain (medical, legal, technical, academic, etc.)

### üìù Exam Prep Mode

- Request quiz questions on specific topics or sections
- Receive immediate feedback on your answers
- Adaptive questioning to avoid repetition
- Customizable for any exam or certification

## Technology Stack

- **LLM**: Together AI or Ollama (select via `LLM_PROVIDER`)
- **Vector Database**: FAISS
- **Embeddings**: HuggingFace Sentence Transformers (configurable via `config/params.env`)
- **Framework**: LangChain
- **PDF Processing**: PyMuPDF (fitz)

## Installation

### Prerequisites

- Python 3.8 or higher
- LLM provider setup:
  - Together AI: API key in `.env` (`TOGETHER_API_KEY`)
  - Ollama: running local server (default `http://localhost:11434`)

### Setup

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd rag-chatbot
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Optional: install pre-commit hooks**

   ```bash
   pip install -r requirements-dev.txt
   pre-commit install
   ```

   To run all hooks on demand:

   ```bash
   pre-commit run --all-files
   ```

4. **Create your secrets file**

   ```bash
   cp .env.example .env
   ```

5. **Create and configure your runtime parameters**

   ```bash
   cp config/params.env.example config/params.env
   ```

   - `config/params.env` (untracked, example provided): set your PDF path, models, chunking, retrieval, and RAGAS settings here.
     - Important: customize the chatbot for your use case by updating:
       - CHATBOT_ROLE
       - USE_CASE
   - Add secrets to `.env` (untracked, example provide): set `TOGETHER_API_KEY=` when using Together AI.
   - Choose your LLM provider in `config/params.env` via `LLM_PROVIDER=together` or `LLM_PROVIDER=ollama`.

   The app always loads `.env` first and `config/params.env` second (no profiles to manage).

## Usage

### Running the Application

From the repo root, after activating your virtualenv:

```bash
python -m src.main
```

### First Run

On the first run, the application will:

1. Process your PDF document
2. Split text into chunks
3. Create embeddings
4. Store vectors in FAISS database

Subsequent runs will load the existing vector database. Delete the database if you want to use a different document.

### Operational Modes

#### Domain Expert Mode (Option 1)

```
‚ùì Your question: What are the key principles of risk-based testing?
üí° Answer: Based on the context, risk-based testing involves...
```

#### Exam Prep Mode (Option 2)

```
üß† Section / topic: Risk Management
‚ùì Your question: What are the main categories of product risks in software testing?
‚ùì Your answer: Functional failures and quality issues
üí° Answer: Good start! The main categories include functional failures...
```

## Project Structure

```
rag-chatbot/
|- README.md
|- requirements.txt
|- .env.example
|- .gitignore
|- config/
|  \- params.env          # Tunable, tracked runtime parameters
|- src/
|  |- main.py             # Main application entry point
|  |- domain_expert.py    # Domain expert logic
|  |- exam_prep.py        # Exam prep logic
|  \- prompts.py          # System and sentence condense prompts
|- data/
|  \- your_document.pdf   # Place any file you want to use for context here
\- faiss_db/              # Vector store - autogenerated upon first run.
```

## Configuration Options

- Secrets live in `.env` (untracked): `TOGETHER_API_KEY` (Together AI only).
- Tunables live in `config/params.env` (tracked): table below.

| Variable          | Default                                         | Description                           |
| ----------------- | ----------------------------------------------- | ------------------------------------- |
| `LLM_PROVIDER`    | `together`                                      | LLM provider: `together` or `ollama`  |
| `OLLAMA_BASE_URL` | `http://localhost:11434`                        | Ollama server URL (Ollama only)       |
| `CHATBOT_ROLE`    | expert tutor                                    | Chatbot's role                        |
| `USE_CASE`        | learn from the provided materials               | Learning goal                         |
| `MODEL_NAME`      | `mistralai/Mistral-7B-Instruct-v0.1`            | LLM model to use                      |
| `PDF_PATH`        | -                                               | Path to your PDF document             |
| `EMBEDDING_MODEL` | `sentence-transformers/paraphrase-MiniLM-L3-v2` | Embedding model                       |
| `DB_DIR`          | `faiss_db`                                      | Directory for vector database         |
| `CHUNK_SIZE`      | `500`                                           | Text chunk size for processing        |
| `CHUNK_OVERLAP`   | `50`                                            | Overlap between text chunks           |
| `RETRIEVAL_K`     | `4`                                             | Number of relevant chunks to retrieve |
| `TEMPERATURE`     | `0.3`                                           | LLM temperature (creativity)          |
| `MAX_TOKENS`      | `512`                                           | Maximum tokens in LLM response        |

## Dependencies

Listed in `requirements.txt` file.

## Commands

During chat sessions:

- Type your question or topic normally
- `mode` - Return to operational mode selection
- `quit`, `exit`, `no`, or `stop` - Exit the application
- `Ctrl+C` - Force quit

## Testing

Standard suite: `pytest` (RAGAS tests are marked and excluded by default, see below). Make sure `.env` and `config/params.env` exist so env loading succeeds.

### Optional RAGAS Evaluations (local only)

Ragas tests are disabled by default when running pytest to avoid breaking CI/CD runs as they need a source document and a golden dataset.

- Relevant variables for paths and file names are all under a Ragas section in `config/params.env`:

  - EVAL_PDF_PATH - source document
  - EVAL_GOLDEN_SET_PATH - golden test dataset
  - EVAL_DB_DIR - where the tests will create the vector store

- Dataset schema: JSON array of objects with `question` and `ground_truth` strings (see `tests/data/golden_set.json.example`).
- How to run: prepare your dataset and vector store locally, then execute `pytest -m ragas`.
- Expected behavior: if files are missing, tests skip/fail with a clear message; no proprietary data is required for CI.

## Troubleshooting

### Common Issues

**PDF Processing Errors**

- Ensure your PDF path is correct in `config/params.env`
- Check that the PDF is readable and not password-protected

**Together AI API Errors**

- Verify your API key is valid and has sufficient credits
- Check network connectivity

**Ollama Errors**

- Ensure the Ollama server is running at `OLLAMA_BASE_URL`
- Verify the model is available locally (e.g., `ollama list`)

**Memory Issues**

- Reduce `CHUNK_SIZE` if processing large documents
- Consider using smaller embedding models

**Vector Database Issues**

- Delete the `faiss_db` directory to rebuild the database
- Ensure sufficient disk space

### Debugging

Enable verbose mode by uncommenting the `verbose=True` lines in the chain configurations:

```python
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True,  # Uncomment for debugging
)
```

## Support

For issues create an issue in the GitHub repository.
