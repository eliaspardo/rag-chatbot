# RAG Chatbot

An AI-powered chatbot using Retrieval-Augmented Generation (RAG) architecture.

## ğŸš€ Features

- **Intelligent Q&A**: Ask questions about the content in your document and get accurate, context-aware answers
- **RAG Architecture**: Combines document retrieval with large language models for precise responses
- **Vector Search**: Uses FAISS for efficient similarity search across document chunks
- **Multiple LLM Support**: Powered by Together AI with Mistral-7B-Instruct model
- **Easy Setup**: Simple installation and configuration process
- **Extensible**: Modular design allows easy customization and extension

## ğŸ› ï¸ Technology Stack

- **LLM**: Together AI
- **Vector Database**: FAISS
- **Embeddings**: Sentence Transformers
- **Framework**: LangChain
- **PDF Processing**: PyMuPDF
- **Language**: Python 3.8+

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- Together AI API key ([Get one here](https://together.ai/))
- PDF document to use for RAG

## ğŸ”§ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/eliaspardo/rag-chatbot.git
cd rag-chatbot
```

### 2. Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up Environment Variables

```bash
cp .env.example .env
```

Edit the `.env` file and add your Together AI API key:

```env
TOGETHER_API_KEY=your_together_ai_api_key_here
```

### 5. Add Your PDF Document

Place your PDF documents in the `data/` directory:

```
data/your_document.pdf
```

### 6. Run the Application

```bash
python src/main.py
```

## ğŸ¯ Usage

Once the application is running, you can ask questions about your document:

```
ğŸ¤– Ready to chat! Try asking:
What is the first chapter about?
A: The first chapter verses about test design - the process of deriving and specifying test cases from test conditions. And how it involves creating detailed test cases that can be executed to verify that the software system meets its requirements...
```

If you want to clean up the vector database, delete the faiss_db folder. E.g. you're changing the documents you want to use for RAG.

## ğŸ—ï¸ Project Structure

```
rag-chatbot/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py            # Main application entry point
â”œâ”€â”€ data/
|   â””â”€â”€ your_document.pdf  # Place any file you want to use for context here
â”œâ”€â”€ faiss_db/              # Vector store - autogenerated upon first run. Delete if you switch documents.
```

## âš™ï¸ Configuration

The application can be configured through environment variables in the `.env` file:

| Variable           | Description                   | Default                                       |
| ------------------ | ----------------------------- | --------------------------------------------- |
| `TOGETHER_API_KEY` | Together AI API key           | Required                                      |
| `PDF_PATH`         | Path to PDF                   | Required                                      |
| `MODEL_NAME`       | LLM model to use              | mistralai/Mistral-7B-Instruct-v0.1            |
| `EMBEDDING_MODEL`  | Embedding model               | sentence-transformers/paraphrase-MiniLM-L3-v2 |
| `DB_DIR`           | Vector database directory     | faiss_db                                      |
| `CHUNK_SIZE`       | Text chunk size               | 500                                           |
| `CHUNK_OVERLAP`    | Chunk overlap                 | 50                                            |
| `RETRIEVAL_K`      | Number of retrieved documents | 4                                             |

## ğŸ“š How It Works

1. **Document Processing**: The PDF is loaded and split into manageable chunks
2. **Embedding Creation**: Text chunks are converted to vector embeddings using Sentence Transformers
3. **Vector Storage**: Embeddings are stored in FAISS for efficient similarity search
4. **Question Processing**: User questions are embedded and used to retrieve relevant document chunks
5. **Answer Generation**: Retrieved context is passed to the LLM to generate accurate answers

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [LangChain](https://langchain.com/) for the RAG framework
- [Together AI](https://together.ai/) for the LLM API
- [Sentence Transformers](https://www.sbert.net/) for embeddings
- [FAISS](https://github.com/facebookresearch/faiss) for vector search

## ğŸ”® Future Enhancements

- [ ] Conversation history and context
- [ ] Customize system prompt
- [ ] Support for multiple documents
- [ ] Multi-language support
- [ ] Batch processing for multiple documents
