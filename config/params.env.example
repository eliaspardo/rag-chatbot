# Domain Configuration - Customize for your use case
CHATBOT_ROLE=Your chatbot role
USE_CASE=what your goal is

# Path to PDF file, supports S3. Comma separated
#PDF_PATH=data/your_pdf_file.pdf,s3://bucket-name/your_pdf_file.pdf
#AWS_ENDPOINT_URL=http://127.0.0.1:4566
#PDF_PATH=s3://your_pdf_file.pdf
#AWS_REGION=us-east-1
#AWS_TEMP_FOLDER=data/temp/



# LLM configuration
# Choose between together|ollama
LLM_PROVIDER=together 
OLLAMA_BASE_URL=http://localhost:11434
# Get from https://docs.together.ai/docs/serverless-models or ollama list
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3 
RETRIEVAL_K=4
TEMPERATURE=0.3
MAX_TOKENS=512

# Database Configuration
DB_DIR=chroma_db

# Preprocessing Configuration
# RAG preprocessor implementation: legacy (PyMuPDF) or docling
RAG_PREPROCESSOR=legacy
# Docling export: markdown or doc_chunks (used when RAG_PREPROCESSOR=docling)
DOCLING_EXPORT_TYPE=doc_chunks
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
CHUNK_SIZE=1500
CHUNK_OVERLAP=150

# EVAL configuration
EVAL_PDF_PATH=data/your_pdf_for_ragas.pdf
EVAL_DB_DIR=tests/data/chroma_db
EVAL_GOLDEN_SET_PATH=tests/data/golden_set.json
EVAL_RESULTS_DIR=tests/artifacts/ragas
# EVAL LLM configuration (defaults to app LLM settings if unset)
# EVAL_LLM_PROVIDER=together
# EVAL_OLLAMA_BASE_URL=http://localhost:11434
# EVAL_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3
# EVAL_TEMPERATURE=0.3
EVAL_MAX_TOKENS=1024
EVAL_TIMEOUT=300
DEEPEVAL_MAX_CONCURRENT=4
EVAL_ANSWER_RELEVANCY_THRESHOLD=0.5
EVAL_FAITHFULNESS_THRESHOLD=0.5
EVAL_PRECISION_THRESHOLD=0.5
EVAL_RECALL_THRESHOLD=0.5
EVAL_ANSWER_RELEVANCY_MIN=0.2
EVAL_FAITHFULNESS_MIN=0.2
EVAL_PRECISION_MIN=0.2
EVAL_ANSWER_RECALL_MIN=0.2
MLFLOW_TRACKING_URI=sqlite:///./mlflow.db
MLFLOW_EXPERIMENT_NAME=RAG Chatbot
